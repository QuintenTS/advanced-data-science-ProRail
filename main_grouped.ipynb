{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae195eaa",
   "metadata": {},
   "source": [
    "Data Cleaning part  \n",
    "run the data cleaning part with 3 years' raw data, and save the output file as \"dataset_1.csv\", \"dataset_2.csv\", \"dataset_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76358b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\n",
    "    'sherlock-export-cm-orders_dag-2022-12-11-2023-12-09.csv',\n",
    "    engine=\"python\", \n",
    "    sep=None           # auto-detects delimiter\n",
    ")\n",
    "# replace the csv name with different raw date file and correct path if needed\n",
    "# Due to the limitation on file size, the raw data is not uploaded on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datacleaning rows\n",
    "data_clean = raw_data.dropna(how=\"all\") #delete complete empty rows\n",
    "data_clean = data_clean[data_clean[\"Procestype\"] != \"ANNULERING\"] # keeps rows where processtype is not ANNULERING (so deletes Annulering)\n",
    "data_clean = data_clean[(data_clean[\"Reactie_type\"] == \"AANGEBODEN\") | (data_clean[\"Reactie_type\"] == \"GEEN_AANBIEDING_MOGELIJK\")] #keeps reaction type: aangeboden and geen mogelijk\n",
    "data_clean = data_clean.dropna(subset=[\"Reactie_type\"]) # delete rows without reaction\n",
    "data_clean = data_clean[data_clean[\"Aanvraagkanaal\"] != \"HANDMATIG\"]  # keeps rows where processtype is not handmatig\n",
    "data_clean = data_clean[data_clean[\"Aanvrager_Verkorting\"] != \"NSR\"] # keeps rows where processtype is not NSR\n",
    "data_clean = data_clean[data_clean[\"Aanvraag_Fase_Planproces\"] != \"JAARDIENST\"] # keeps rows where processtype is not JAARDIENST\n",
    "data_clean_rows = data_clean.drop_duplicates(subset=[\"Ordernummer\"], keep=\"last\") # keep only row with most recent ordernummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datacleaning columns \"AANVRAAG\"\n",
    "data_clean = data_clean_rows.drop(columns=['*bronxml', '*Order_Orderelementen_teller',\n",
    "       '*Order_Orderelementen_aangevraagdedagen_teller',\n",
    "       '*Order_Reactielementen_teller', 'Ordergroep_Groepsid', 'Ordergroep_Omschrijving',\n",
    "       'Ordergroep_Opmerkingen_Intern', 'Ordergroep_ingeschatteUren',\n",
    "       'Ordergroep_gespendeerdeUren', 'Ordergroep_resterendeUren',\n",
    "       'Orderversienummer', 'Aanvraagkanaal', 'Ordercategorie', 'Omschrijving',\n",
    "       'Opmerkingen_Intern', 'Status', 'Taastatus', 'Aanvraag_Grensafstemming',\n",
    "       'Status_Grensafstemming', 'Behandelaar',\n",
    "       'VerwachtePublicatiedatum', 'AfgesprokenPublicatiedatum',\n",
    "       'Aanvraag_Case_Reference', 'Aanvraag_Identificatie', 'Aanvraag_Variant',\n",
    "       'Aanvraag_Soort', 'Aanvraag_Berichttype','Aanvraag_Fase_Planproces',\n",
    "        'Oorspronkelijk_Treinnummer',\n",
    "        'AangevraagdeDatumStatus',\n",
    "       'Aanvraag_Eerste_Locatie_Treintype',\n",
    "       'Aanvraag_Eerste_Locatie_Rijkarakteristiek',\n",
    "       'Aanvraag_Eerste_Locatie_BVregelingen',\n",
    "       'Aanvraag_Eerste_Locatie_Opmerkingen',\n",
    "       'Aanvraag_Eerste_Locatie_Materieelsoort',\n",
    "       'Aanvraag_Eerste_Locatie_Materieeltype','Aanvraag_Eerste_Locatie_Dienstregelingsnelheid',\n",
    "        'Aanvraag_Eerste_Locatie_Remstand',\n",
    "       'Aanvraag_Eerste_Locatie_Rempercentage',\n",
    "       'Aanvraag_Eerste_Locatie_Belastingklasse',\n",
    "       'Aanvraag_Eerste_Locatie_Profielcodes', 'Aanvraag_Eerste_Locatie_Behandelduur',\n",
    "        'Aanvraag_Eerste_Locatie_Gewenste_Vertrektijd','Aanvraag_Eerste_Locatie_Behandelduur',\n",
    "        'Aanvraag_Laatste_Locatie_Gewenste_Aankomsttijd','Aanvraag_Laatste_Locatie_Behandelduur',\n",
    "        'Reactie_Variant', \n",
    "        'Aanvraag_Treinwijzingen', 'Reactie_Dienstregelingjaar',\n",
    "       'Reactie_Vervoerder_Afkorting', 'Reactie_Treinnummer',\n",
    "       'Aanvraag_Verwerkingdagen_Aangebodendag',\n",
    "       'Reactie_Eerste_Activiteit_Treintype',\n",
    "       'Reactie_Eerste_Activiteit_Rijkarakteristiek',\n",
    "       'Reactie_Eerste_Activiteit_BVRegelingen',\n",
    "       'Reactie_Eerste_Activiteit_Materieelsoort',\n",
    "       'Reactie_Eerste_Activiteit_Materieeltype',\n",
    "       'Reactie_Eerste_Activiteit_Tractievorm',\n",
    "       'Reactie_Eerste_Activiteit_Dienstregelingssnelheid',\n",
    "       'Reactie_Eerste_Activiteit_Remstand',\n",
    "       'Reactie_Eerste_Activiteit_RemPercentage',\n",
    "       'Reactie_Eerste_Activiteit_Belastingklasse',\n",
    "       'Reactie_Eerste_Activiteit_Profielcodes',\n",
    "       'Reactie_Eerste_Activiteit_Toelichting',\n",
    "       'Reactie_Eerste_Activiteit_DienstregelpuntSpoor',\n",
    "       'Reactie_Eerste_Activiteit_Treinactiviteit',\n",
    "       'Reactie_Eerste_Activiteit_Tijdstip_offset',\n",
    "       'Reactie_Eerste_Activiteit_Opmerkingen',\n",
    "       'Reactie_Laatste_Activiteit_DienstregelpuntSpoor',\n",
    "       'Reactie_Laatste_Activiteit_Activiteitsoort',\n",
    "       'Reactie_Laatste_Activiteit_Tijdstip_Offset',\n",
    "       'Reactie_Laatste_Activiteit_Opmerkingen',\n",
    "       'IngeschatteUren', 'GespendeerdeUren', 'ResterendeUren', 'Aanvraag_Eerste_Locatie_DienstregelpuntSpoor', 'Aanvraag_Laatste_Locatie_DienstregelpuntSpoor','Aanvraag_vervoerder_verkorting','Aanvrager_Verkorting', 'Unnamed: 109'])\n",
    "\n",
    "#All reaction columns deleted except 'Reactie_type' \n",
    "print(data_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Number_Of_Stops_requested\"] = data_clean[\"Aanvraag_Routelint\"].str.split(\"-\").str.len() # create new column with number of stops\n",
    "data_clean[\"Aanvraag_Indienmoment\"] = pd.to_datetime(data_clean[\"Aanvraag_Indienmoment\"]) # make info in column datetime info\n",
    "data_clean[\"Aanvraag_Day\"] = data_clean[\"Aanvraag_Indienmoment\"].dt.day_name() # create new column with day of week of request \n",
    "data_clean[\"Aanvraag_time\"] = data_clean[\"Aanvraag_Indienmoment\"].dt.time # create new column with time of request\n",
    "\n",
    "#time range request first location\n",
    "data_clean[\"Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd\"] = pd.to_datetime(data_clean[\"Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd\"], format=\"%H:%M:%S\") #convert to time\n",
    "data_clean[\"Aanvraag_Eerste_Locatie_Laatste_Vertrektijd\"] = pd.to_datetime(data_clean[\"Aanvraag_Eerste_Locatie_Laatste_Vertrektijd\"], format=\"%H:%M:%S\") #convert to time\n",
    "data_clean[\"Aanvraag_time_range_Eerste_Locatie_Vertrektijd\"] = (data_clean['Aanvraag_Eerste_Locatie_Laatste_Vertrektijd'] - data_clean['Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd']).dt.total_seconds()/60\n",
    "data_clean.loc[data_clean[\"Aanvraag_time_range_Eerste_Locatie_Vertrektijd\"] < 0,\n",
    "    \"Aanvraag_time_range_Eerste_Locatie_Vertrektijd\"] += 1440 \n",
    "\n",
    "\n",
    "\n",
    "#time range request last location\n",
    "data_clean[\"Aanvraag_Laatste_Locatie_Vroegste_Aankomsttijd\"] = pd.to_datetime(data_clean[\"Aanvraag_Laatste_Locatie_Vroegste_Aankomsttijd\"], format=\"%H:%M:%S\")#convert to time\n",
    "data_clean[\"Aanvraag_Laatste_Locatie_Laatste_Aankomsttijd\"] = pd.to_datetime(data_clean[\"Aanvraag_Laatste_Locatie_Laatste_Aankomsttijd\"], format=\"%H:%M:%S\") #convert to time\n",
    "data_clean[\"Aanvraag_time_range_Laatste_Locatie_Aankomsttijd\"] = (data_clean[\"Aanvraag_Laatste_Locatie_Laatste_Aankomsttijd\"] - data_clean[\"Aanvraag_Laatste_Locatie_Vroegste_Aankomsttijd\"]).dt.total_seconds()/60\n",
    "data_clean.loc[data_clean[\"Aanvraag_time_range_Laatste_Locatie_Aankomsttijd\"] < 0,\n",
    "    \"Aanvraag_time_range_Laatste_Locatie_Aankomsttijd\"] += 1440 \n",
    "    \n",
    "\n",
    "\n",
    "# New col for behandelingen, if stabling is needed 1, else 0\n",
    "data_clean[\"Aanvraag_Eerste_Locatie_Opstellen_True\"] = data_clean[\"Aanvraag_Eerste_Locatie_Behandelingen\"].str.contains(\"Opstellen\", na=False).astype(int)\n",
    "data_clean[\"Aanvraag_Laatste_Locatie_Opstellen_True\"] = data_clean[\"Aanvraag_Laatste_Locatie_Behandelingen\"].str.contains(\"Opstellen\", na=False).astype(int)\n",
    "\n",
    "\n",
    "#data_clean['Time_request_operation'] = data_clean[\"Aanvraag_Indienmoment\"] - data_clean[\"Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd\"]\n",
    "\n",
    "\n",
    "# drop used columns\n",
    "data_clean = data_clean.drop(columns=[\"Aanvraag_Routelint\", \"Aanvraag_Eerste_Locatie_Behandelingen\", \"Aanvraag_Laatste_Locatie_Behandelingen\"])\n",
    "\n",
    "display(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate indienrange\n",
    "# 1) Parse day & time with coercion\n",
    "data_clean['first_day'] = (\n",
    "    data_clean['Aanvraag_Dagen_Specifiek']\n",
    "    .astype(str).str.split(',').str[0].str.strip()\n",
    ")\n",
    "\n",
    "data_clean['Aanvraag_day'] = pd.to_datetime(data_clean['first_day'], errors='coerce')\n",
    "\n",
    "# Parse the earliest time column to datetime, then keep only the time part\n",
    "dt_full = pd.to_datetime(data_clean['Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd'], errors='coerce')\n",
    "data_clean['Aanvraag_time'] = dt_full.dt.time  # will be NaT where parsing failed\n",
    "\n",
    "# 2) Filter out rows with missing day or time\n",
    "mask_valid = data_clean['Aanvraag_day'].notna() & data_clean['Aanvraag_time'].notna()\n",
    "bad_rows = data_clean.loc[~mask_valid].copy()   # optional: inspect these\n",
    "print(f\"Dropping {(~mask_valid).sum()} rows with missing day or time\")\n",
    "\n",
    "data_clean = data_clean.loc[mask_valid].copy()\n",
    "\n",
    "# 3) Safely combine into a full timestamp\n",
    "data_clean['Aanvraag_Operation_Specifiek'] = pd.to_datetime(\n",
    "    data_clean['Aanvraag_day'].dt.strftime('%Y-%m-%d') + ' ' + data_clean['Aanvraag_time'].astype(str),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# If any still failed (weird edge-case strings), drop those too:\n",
    "still_bad = data_clean['Aanvraag_Operation_Specifiek'].isna()\n",
    "if still_bad.any():\n",
    "    print(f\"Dropping {still_bad.sum()} rows with unparseable combined datetime\")\n",
    "    data_clean = data_clean.loc[~still_bad].copy()\n",
    "\n",
    "#calculate difference between the requested operation moment and the moment the request is submitted --> how many hours:minutes:seconds in advance if hours >24 it is more than 1 day in advance\n",
    "data_clean[\"Difference_request_operation_time\"]  = data_clean[\"Aanvraag_Operation_Specifiek\"] - data_clean['Aanvraag_Indienmoment']\n",
    "data_clean[\"Difference_request_operation_time\"] = data_clean[\"Difference_request_operation_time\"].apply(\n",
    "    lambda x: f\"{int(x.total_seconds() // 3600):02d}:{int((x.total_seconds() % 3600) // 60):02d}:{int(x.total_seconds() % 60):02d}\"\n",
    "    if pd.notnull(x) else None)\n",
    "\n",
    "# calculate process time of processing a requesrt in hours:minutes:seconds\n",
    "data_clean[\"Process_time_date\"] = pd.to_datetime(data_clean['EersteBehandelTijdstip']) - data_clean['Aanvraag_Indienmoment']\n",
    "\n",
    "data_clean[\"Process_time\"] = data_clean[\"Process_time_date\"].apply(\n",
    "    lambda x: f\"{int(x.total_seconds() // 3600):02d}:{int((x.total_seconds() % 3600) // 60):02d}:{int(x.total_seconds() % 60):02d}\"\n",
    "    if pd.notnull(x) else None)\n",
    "\n",
    "#calculate number of stops responded\n",
    "data_clean[\"Number_Of_Stops_response\"] = data_clean[\"Reactie_Routelint\"].str.split(\"-\").str.len() # create new column with number of stops\n",
    "\n",
    "#drop used 'temporary' columns \n",
    "data_clean = data_clean.drop(columns=['Aanvraag_time', 'Aanvraag_day', 'first_day', \"Process_time_date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dataset as csv\n",
    "\n",
    "data_clean.to_csv(\"dataset_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e34f0d",
   "metadata": {},
   "source": [
    "Data Combination  \n",
    "Combine 3 separate datasets from 3 years into 1 single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- INPUTS ---\n",
    "files = [\n",
    "    \"dataset_1.csv\",\n",
    "    \"dataset_2.csv\",\n",
    "    \"dataset_3.csv\"\n",
    "]\n",
    "output_path = \"cleaned_dataset.csv\"\n",
    "\n",
    "read_opts = dict(sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Read all CSVs\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, **read_opts)\n",
    "    df[\"__source_file\"] = Path(f).name  # keep provenance\n",
    "    dfs.append(df)\n",
    "\n",
    "# Align columns in case of slight mismatches\n",
    "all_cols = sorted(set().union(*[d.columns for d in dfs]))\n",
    "dfs = [d.reindex(columns=all_cols) for d in dfs]\n",
    "\n",
    "# Stack (row-bind)\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined = combined.drop_duplicates()\n",
    "\n",
    "# Save\n",
    "combined.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(combined):,} rows to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fa2c6",
   "metadata": {},
   "source": [
    "Fill the missing information in WIJZIGING requests from the matching NIEUW requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_dataset_23_10.csv\")\n",
    "keys = [\"Aanvraag_Treinnummer\", \"Aanvraag_Dagen_Specifiek\"]\n",
    "\n",
    "# Keep original order\n",
    "df = df.copy()\n",
    "df[\"_row_order\"] = range(len(df))\n",
    "\n",
    "w = df[df[\"Procestype\"].eq(\"WIJZIGING\")].copy()\n",
    "n = df[df[\"Procestype\"].eq(\"NIEUW\")].copy()\n",
    "\n",
    "# Aggregate NIEUW by key -> first non-null per column\n",
    "non_key_cols = [c for c in df.columns if c not in (set(keys) | {\"Procestype\", \"_row_order\"})]\n",
    "\n",
    "def first_non_null(s):\n",
    "    s2 = s.dropna()\n",
    "    return s2.iloc[0] if not s2.empty else pd.NA\n",
    "\n",
    "n_agg = (\n",
    "    n.groupby(keys, dropna=False)[non_key_cols]\n",
    "      .agg(first_non_null)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Join aggregated NIEUW onto WIJZIGING and fill gaps\n",
    "w_filled = w.merge(n_agg, on=keys, how=\"left\", suffixes=(\"\", \"_FROM_NIEUW\"))\n",
    "for c in non_key_cols:\n",
    "    w_filled[c] = w_filled[c].combine_first(w_filled[f\"{c}_FROM_NIEUW\"])\n",
    "w_filled = w_filled[[c for c in w.columns]]  # drop helper columns\n",
    "\n",
    "# Reassemble the full dataframe\n",
    "out = pd.concat([w_filled, df[~df[\"Procestype\"].eq(\"WIJZIGING\")]], ignore_index=True)\n",
    "out = out.sort_values(\"_row_order\").drop(columns=\"_row_order\")\n",
    "\n",
    "# Save / inspect\n",
    "out.to_csv(\"cleaned_dataset_filled.csv\", index=False)\n",
    "print(\"Filled WIJZIGING rows:\", w.shape[0])\n",
    "print(out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b90291",
   "metadata": {},
   "source": [
    "Numeric dataset transformation part (Encoding the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b82409",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(\n",
    "    'cleaned_dataset_filled.csv',\n",
    "    engine=\"python\", \n",
    "    sep=None           # auto-detects delimiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all empty columns (columns where *all* values are missing)\n",
    "cleaned_data = cleaned_data.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop rows where the target column is missing/empty\n",
    "cleaned_data = cleaned_data[ cleaned_data[\"Aanvraag_Eerste_Locatie_Dienstregelpunt\"].notna() ]\n",
    "cleaned_data = cleaned_data[ cleaned_data[\"Aanvraag_Eerste_Locatie_Dienstregelpunt\"].astype(str).str.strip() != \"\" ]\n",
    "\n",
    "print(\"Cleaned dataset shape:\", cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd497938",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = \"Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd\"\n",
    "\n",
    "# Ensure datetimelike\n",
    "cleaned_data[col1] = pd.to_datetime(cleaned_data[col1], errors=\"coerce\")\n",
    "\n",
    "# Extract hour (0–23)\n",
    "cleaned_data[\"Hour_group\"] = cleaned_data[col1].dt.hour\n",
    "\n",
    "print(cleaned_data[[col1, \"Hour_group\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total missing values in Hour_group\n",
    "missing_count = cleaned_data[\"Hour_group\"].isna().sum()\n",
    "print(f\"Total missing Hour_group values: {missing_count}\")\n",
    "\n",
    "# Show the rows where Hour_group is missing\n",
    "missing_rows = cleaned_data[cleaned_data[\"Hour_group\"].isna()][\n",
    "    [\"Aanvraag_Eerste_Locatie_Vroegste_Vertrektijd\", \"Hour_group\"]\n",
    "]\n",
    "\n",
    "print(missing_rows.head(20))  # show first 20 missing cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out those rows without requested earliest departure time\n",
    "data_NIEUW = cleaned_data.dropna(subset=[\"Hour_group\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4879183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the hours of day in a cyclical way, sothat 0:00 is close to 23:00\n",
    "data_NIEUW[\"Hour_sin\"] = np.sin(2 * np.pi * data_NIEUW[\"Hour_group\"] / 24)\n",
    "data_NIEUW[\"Hour_cos\"] = np.cos(2 * np.pi * data_NIEUW[\"Hour_group\"] / 24)\n",
    "\n",
    "# Show example\n",
    "print(data_NIEUW[[\"Hour_group\", \"Hour_sin\", \"Hour_cos\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af804bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "col2 = \"Aanvraag_Indienmoment\"\n",
    "\n",
    "# Ensure datetimelike\n",
    "data_NIEUW[col2] = pd.to_datetime(data_NIEUW[col2], errors=\"coerce\")\n",
    "\n",
    "# Extract hour (0–23)\n",
    "data_NIEUW[\"Submission_hour_group\"] = data_NIEUW[col2].dt.hour\n",
    "\n",
    "print(data_NIEUW[[col2, \"Submission_hour_group\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff900806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the reaction by prorail into binary numbers\n",
    "data_NIEUW[\"Approved\"] = data_NIEUW[\"Reactie_type\"].map({\n",
    "    \"AANGEBODEN\": 1,\n",
    "    \"GEEN_AANBIEDING_MOGELIJK\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert hh:mm:ss to timedelta\n",
    "data_NIEUW[\"Process_time\"] = pd.to_timedelta(data_NIEUW[\"Process_time\"])\n",
    "\n",
    "# Convert to numeric (total seconds)\n",
    "data_NIEUW[\"Process_time_sec\"] = data_NIEUW[\"Process_time\"].dt.total_seconds()\n",
    "\n",
    "print(data_NIEUW[\"Process_time_sec\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the information from the corresponding columns\n",
    "data_NIEUW[\"Requested_weight\"] = data_NIEUW[\"Aanvraag_Eerste_Locatie_MaxGewicht\"]\n",
    "data_NIEUW[\"Requested_length\"] = data_NIEUW[\"Aanvraag_Eerste_Locatie_MaxLengte\"]\n",
    "data_NIEUW[\"Requested_speed\"] = data_NIEUW[\"Aanvraag_Eerste_Locatie_MaxSnelheid\"]\n",
    "data_NIEUW[\"Stabling\"] = data_NIEUW[\"Aanvraag_Laatste_Locatie_Opstellen_True\"]\n",
    "data_NIEUW[\"Tolerance\"] = data_NIEUW[\"Aanvraag_time_range_Eerste_Locatie_Vertrektijd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the days of week in a cyclical way, so that Monday is close to Sunday\n",
    "mapping = {\"Monday\":0, \"Tuesday\":1, \"Wednesday\":2, \n",
    "           \"Thursday\":3, \"Friday\":4, \"Saturday\":5, \"Sunday\":6}\n",
    "data_NIEUW[\"Day_num\"] = data_NIEUW[\"Aanvraag_Day\"].map(mapping)\n",
    "data_NIEUW[\"Day_sin\"] = np.sin(2 * np.pi * data_NIEUW[\"Day_num\"] / 7)\n",
    "data_NIEUW[\"Day_cos\"] = np.cos(2 * np.pi * data_NIEUW[\"Day_num\"] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "ORIGIN_COL = \"Aanvraag_Eerste_Locatie_Dienstregelpunt\"\n",
    "DEST_COL   = \"Aanvraag_Laatste_Locatie_Dienstregelpunt\"\n",
    "GROUPS_XLSX = \"Grouped_locations.xlsx\"\n",
    "\n",
    "# 1) Build location -> group mapping from your wide Excel (headers are group names)\n",
    "wide = pd.read_excel(GROUPS_XLSX)\n",
    "wide = wide.applymap(lambda x: str(x).strip() if pd.notna(x) else x)\n",
    "\n",
    "long = wide.melt(value_name=\"Location\", var_name=\"Group\").dropna(subset=[\"Location\"])\n",
    "long[\"Location_norm\"] = long[\"Location\"].str.strip().str.lower()\n",
    "long[\"Group\"] = long[\"Group\"].str.strip()\n",
    "loc2grp = dict(zip(long[\"Location_norm\"], long[\"Group\"]))\n",
    "\n",
    "# Preserve header order; ensure \"Nederland\" is present in the list\n",
    "group_labels = [str(c).strip() for c in wide.columns.tolist()]\n",
    "if \"Nederland\" not in group_labels:\n",
    "    group_labels.append(\"Nederland\")\n",
    "\n",
    "# 2) Map to groups; fill any unmapped with \"Nederland\"\n",
    "def map_to_group(series):\n",
    "    return series.astype(str).str.strip().str.lower().map(loc2grp)\n",
    "\n",
    "data_NIEUW[\"origin_group\"] = map_to_group(data_NIEUW[ORIGIN_COL]).fillna(\"Nederland\")\n",
    "data_NIEUW[\"dest_group\"]   = map_to_group(data_NIEUW[DEST_COL]).fillna(\"Nederland\")\n",
    "\n",
    "# (Optional) see how many were filled as Nederland\n",
    "n_origin_filled = (data_NIEUW[\"origin_group\"] == \"Nederland\") & \\\n",
    "                  ~data_NIEUW[ORIGIN_COL].astype(str).str.strip().str.lower().isin(loc2grp.keys())\n",
    "n_dest_filled   = (data_NIEUW[\"dest_group\"] == \"Nederland\") & \\\n",
    "                  ~data_NIEUW[DEST_COL].astype(str).str.strip().str.lower().isin(loc2grp.keys())\n",
    "print(\"Unmapped→Nederland counts | origin:\", int(n_origin_filled.sum()), \"dest:\", int(n_dest_filled.sum()))\n",
    "\n",
    "# 3) Create O–D pair labels and one-hot (ensure full 4×4 grid)\n",
    "data_NIEUW[\"OD_pair\"] = data_NIEUW[\"origin_group\"] + \" - \" + data_NIEUW[\"dest_group\"]\n",
    "od_dummies = pd.get_dummies(data_NIEUW[\"OD_pair\"], dtype=\"int64\")\n",
    "\n",
    "all_pairs = [f\"{o} - {d}\" for o, d in product(group_labels, group_labels)]\n",
    "for col in all_pairs:\n",
    "    if col not in od_dummies.columns:\n",
    "        od_dummies[col] = 0\n",
    "od_dummies = od_dummies[all_pairs]\n",
    "\n",
    "# 4) Merge back (drop raw O/D if desired)\n",
    "data_NIEUW_encoded = pd.concat(\n",
    "    [data_NIEUW.drop(columns=[ORIGIN_COL, DEST_COL, \"origin_group\", \"dest_group\", \"OD_pair\"], errors=\"ignore\"),\n",
    "     od_dummies],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"O–D columns:\", list(od_dummies.columns))\n",
    "print(\"Encoded shape:\", data_NIEUW_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a51a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate all the columns into a numeric dataset for model processing\n",
    "numeric_dataset = pd.concat([data_NIEUW[[\"Requested_weight\"]],\n",
    "                             data_NIEUW[[\"Requested_length\"]],\n",
    "                             data_NIEUW[[\"Requested_speed\"]],\n",
    "                             data_NIEUW[[\"Stabling\"]], \n",
    "                             data_NIEUW[[\"Tolerance\"]],\n",
    "                             data_NIEUW[[\"Process_time_sec\"]],\n",
    "                             data_NIEUW[[\"Day_sin\"]],\n",
    "                             data_NIEUW[[\"Day_cos\"]], \n",
    "                             data_NIEUW[[\"Hour_sin\"]],\n",
    "                             data_NIEUW[[\"Hour_cos\"]],\n",
    "                             od_dummies,\n",
    "                             data_NIEUW[[\"Approved\"]]], \n",
    "                             axis=1)\n",
    "\n",
    "print(numeric_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any NaNs anywhere?\n",
    "numeric_dataset.isna().values.any()\n",
    "\n",
    "# Count NaNs per column\n",
    "numeric_dataset.isna().sum()\n",
    "\n",
    "# Total number of NaNs in the whole DataFrame\n",
    "numeric_dataset.isna().sum().sum()\n",
    "\n",
    "# Columns that contain any NaN\n",
    "cols_with_nan = numeric_dataset.columns[numeric_dataset.isna().any()].tolist()\n",
    "\n",
    "# Rows that contain any NaN (index list)\n",
    "rows_with_nan = numeric_dataset.index[numeric_dataset.isna().any(axis=1)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_summary = (\n",
    "    numeric_dataset.isna().mean().rename(\"pct_nan\").to_frame()\n",
    "    .assign(n_nan=numeric_dataset.isna().sum())\n",
    "    .query(\"n_nan > 0\")\n",
    "    .sort_values(\"pct_nan\", ascending=False)\n",
    ")\n",
    "print(na_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dataset = numeric_dataset.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0438d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to new CSV file\n",
    "numeric_dataset.to_csv(\"numeric_dataset_grouped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d68c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "n_total = len(numeric_dataset)\n",
    "n_approved = int(numeric_dataset[\"Approved\"].sum())      # works for 0/1 or booleans\n",
    "n_rejected = n_total - n_approved\n",
    "\n",
    "# Overall approval probability\n",
    "approval_rate = numeric_dataset[\"Approved\"].mean()       # fraction of rows with Approved==1 (or True)\n",
    "\n",
    "print(\"Total:\", n_total)\n",
    "print(\"Approved:\", n_approved)\n",
    "print(\"Rejected:\", n_rejected)\n",
    "print(\"Approval rate:\", approval_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d31243",
   "metadata": {},
   "source": [
    "Logistic Regression part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "numeric_data = pd.read_csv(\n",
    "    'numeric_dataset_grouped.csv',\n",
    "    engine=\"python\", \n",
    "    sep=None           # auto-detects delimiter\n",
    ")\n",
    "\n",
    "#(IMPORTANT) sample the dataset if ran out of RAM\n",
    "numeric_subset = numeric_data.sample(n=100000, random_state=42)\n",
    "numeric_subset = numeric_subset.loc[:, (numeric_subset != 0).any(axis=0)]\n",
    "\n",
    "y = numeric_subset['Approved'].values\n",
    "X = numeric_subset.drop(columns=['Approved', 'Process_time_sec']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92660b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logit\", LogisticRegression(solver=\"saga\", max_iter=5000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"logit__penalty\": [\"l2\", \"l1\"],\n",
    "    \"logit__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"logit__class_weight\": [None, \"balanced\"],   # try if classes are imbalanced\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",   # ranking metric, good for probabilities\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "best_clf = gs.best_estimator_\n",
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate probability quality\n",
    "proba_test = best_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, proba_test))\n",
    "print(\"PR AUC:\", average_precision_score(y_test, proba_test))  # useful for imbalance\n",
    "print(\"Log Loss:\", log_loss(y_test, proba_test))               # proper scoring rule\n",
    "print(\"Brier Score:\", brier_score_loss(y_test, proba_test))    # calibration-focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93604b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Calibration check\n",
    "frac_true, frac_pred = calibration_curve(y_test, proba_test, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure()\n",
    "plt.plot(frac_pred, frac_true, marker=\"o\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed probability\")\n",
    "plt.title(\"Calibration curve (uncalibrated)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5796c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "calibrated = CalibratedClassifierCV(best_clf, method=\"isotonic\", cv=5)  # or method=\"sigmoid\"\n",
    "calibrated.fit(X_train, y_train)\n",
    "\n",
    "proba_test_cal = calibrated.predict_proba(X_test)[:, 1]\n",
    "print(\"Brier (calibrated):\", brier_score_loss(y_test, proba_test_cal))\n",
    "print(\"Log Loss (calibrated):\", log_loss(y_test, proba_test_cal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncalibrated\n",
    "frac_true_u, frac_pred_u = calibration_curve(y_test, proba_test, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "# Calibrated\n",
    "frac_true_c, frac_pred_c = calibration_curve(y_test, proba_test_cal, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(frac_pred_u, frac_true_u, marker=\"o\", label=\"Uncalibrated\")\n",
    "plt.plot(frac_pred_c, frac_true_c, marker=\"o\", label=\"Calibrated (isotonic)\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed probability\")\n",
    "plt.title(\"Calibration curve: before vs after\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7607800",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"Approved\"\n",
    "\n",
    "# 1) Feature columns (keeps original order)\n",
    "feature_cols = numeric_data.drop(columns=[target_col]).columns.tolist()\n",
    "\n",
    "# 2) Train split using those columns\n",
    "X = numeric_data[feature_cols].to_numpy()\n",
    "y = numeric_data[target_col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca80355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the calibrated model\n",
    "import joblib, sklearn, json, time\n",
    "\n",
    "ORIGIN_COL = \"Aanvraag_Eerste_Locatie_Dienstregelpunt\"\n",
    "DEST_COL   = \"Aanvraag_Laatste_Locatie_Dienstregelpunt\"\n",
    "\n",
    "artifact = {\n",
    "    \"model\": calibrated,                    # or best_clf if you’re not using calibration\n",
    "    \"feature_cols\": feature_cols,           # exact training column order\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "joblib.dump(artifact, \"logit_calibrated.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b22673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect coefficients\n",
    "coef = best_clf.named_steps[\"logit\"].coef_.ravel()\n",
    "intercept = best_clf.named_steps[\"logit\"].intercept_[0]\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef\": coef,\n",
    "    \"odds_ratio\": np.exp(coef)\n",
    "}).sort_values(\"coef\", ascending=False)\n",
    "coef_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mude-week-2-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
